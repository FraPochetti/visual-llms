import { GoogleGenerativeAI } from '@google/generative-ai';
import { GoogleGenAI, PersonGeneration } from '@google/genai';

if (!process.env.GEMINI_API_KEY) {
    throw new Error('GEMINI_API_KEY is not set in environment variables');
}

// Initialize the Gemini API client
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// Initialize the Imagen client
const imagenClient = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });

/**
 * Generate an image from a text prompt using Gemini Nano Banana
 * Uses Gemini 2.5 Flash Image model
 */
export async function generateImage(prompt: string): Promise<{
    imageData: string;
    mimeType: string;
}> {
    try {
        // Use Gemini 2.5 Flash Image (Nano Banana)
        const model = genAI.getGenerativeModel({
            model: 'gemini-2.5-flash-image'
        });

        const result = await model.generateContent(prompt);

        const response = result.response;

        // Extract image data from response parts
        if (response.candidates && response.candidates[0]?.content?.parts) {
            const parts = response.candidates[0].content.parts;

            // Look for inline data (images)
            for (const part of parts) {
                if ('inlineData' in part && part.inlineData) {
                    return {
                        imageData: part.inlineData.data,
                        mimeType: part.inlineData.mimeType || 'image/png',
                    };
                }
            }
        }

        throw new Error('No image data found in Gemini response');
    } catch (error) {
        console.error('Error generating image with Gemini:', error);
        throw error;
    }
}

/**
 * Generate an image from a text prompt using Imagen 4 Ultra
 * Google's highest-fidelity image generation model
 */
export async function generateImageWithImagen4(prompt: string): Promise<{
    imageData: string;
    mimeType: string;
}> {
    try {
        const response = await imagenClient.models.generateImages({
            model: 'imagen-4.0-ultra-generate-001',
            prompt,
            config: {
                numberOfImages: 1,
                aspectRatio: "1:1",
                imageSize: "2K",
                personGeneration: PersonGeneration.ALLOW_ADULT
            }
        });

        if (!response.generatedImages || response.generatedImages.length === 0) {
            throw new Error('No images generated by Imagen 4');
        }

        const generatedImage = response.generatedImages[0];
        if (!generatedImage?.image?.imageBytes) {
            throw new Error('Invalid image data from Imagen 4');
        }

        const imageBytes = generatedImage.image.imageBytes;

        return {
            imageData: imageBytes,
            mimeType: 'image/png',
        };
    } catch (error) {
        console.error('Error generating image with Imagen 4:', error);
        throw error;
    }
}

/**
 * Edit an existing image using Gemini Nano Banana
 * Maintains consistent likenesses and supports advanced edits
 */
export async function editImage(
    imageBuffer: Buffer,
    instruction: string,
    mimeType: string = 'image/png'
): Promise<{
    imageData: string;
    mimeType: string;
}> {
    try {
        // Use Gemini 2.5 Flash Image (Nano Banana)
        const model = genAI.getGenerativeModel({
            model: 'gemini-2.5-flash-image'
        });

        // Convert buffer to base64
        const imageBase64 = imageBuffer.toString('base64');

        const result = await model.generateContent([
            {
                inlineData: {
                    data: imageBase64,
                    mimeType: mimeType,
                },
            },
            { text: instruction }
        ]);

        const response = result.response;

        // Debug: Log what we received
        console.log('Gemini response candidates:', response.candidates?.length);
        if (response.candidates && response.candidates[0]) {
            console.log('First candidate parts:', response.candidates[0].content?.parts?.length);
            console.log('Parts types:', response.candidates[0].content?.parts?.map((p: any) =>
                Object.keys(p).join(',')
            ));
        }

        // Extract edited image from response
        if (response.candidates && response.candidates[0]?.content?.parts) {
            const parts = response.candidates[0].content.parts;

            for (const part of parts) {
                // Check for inline data (images)
                if ('inlineData' in part && part.inlineData) {
                    return {
                        imageData: part.inlineData.data,
                        mimeType: part.inlineData.mimeType || mimeType,
                    };
                }

                // Log if we got text instead
                if ('text' in part && part.text) {
                    console.log('Got text response from Gemini:', part.text.substring(0, 200));
                }
            }
        }

        // More detailed error with what we actually got
        const receivedTypes = response.candidates?.[0]?.content?.parts?.map((p: any) =>
            Object.keys(p)[0]
        ).join(', ') || 'no parts';

        throw new Error(`No edited image data found in Gemini response. Received: ${receivedTypes}`);
    } catch (error) {
        console.error('Error editing image with Gemini:', error);
        throw error;
    }
}

/**
 * Get text response from Gemini (for chat features)
 */
export async function generateText(prompt: string): Promise<string> {
    try {
        const model = genAI.getGenerativeModel({
            model: 'gemini-2.0-flash-exp'
        });

        const result = await model.generateContent({
            contents: [{
                role: 'user',
                parts: [{ text: prompt }]
            }],
            generationConfig: {
                temperature: 1,
                topK: 40,
                topP: 0.95,
                maxOutputTokens: 8192,
            },
        });

        const response = result.response;
        return response.text();
    } catch (error) {
        console.error('Error generating text with Gemini:', error);
        throw error;
    }
}

/**
 * Generate a video from a text prompt using Veo 3.1
 * Supports two modes: text-only and single-frame (image-to-video)
 */
export async function generateVideo(
    prompt: string,
    options?: {
        firstFrame?: Buffer;
        onProgress?: (seconds: number) => void;
    }
): Promise<{
    videoData: Buffer;
    mimeType: string;
    duration: number;
    resolution: string;
}> {
    try {
        console.log('Starting video generation with Veo 3.1...');
        console.log('Prompt:', prompt);
        console.log('Has first frame:', !!options?.firstFrame);

        // For image-to-video, pass image bytes directly
        let operation: any;

        if (options?.firstFrame) {
            console.log('Passing image bytes (base64) directly to Veo 3.1...');

            // Convert Buffer to base64 string
            const firstFrameBase64 = options.firstFrame.toString('base64');

            // Single-frame mode - use image parameter
            operation = await imagenClient.models.generateVideos({
                model: 'veo-3.1-generate-preview',
                prompt,
                image: {
                    imageBytes: firstFrameBase64,
                    mimeType: 'image/png'
                }
            });
        } else {
            // Text-only mode - use SDK
            operation = await imagenClient.models.generateVideos({
                model: 'veo-3.1-generate-preview',
                prompt,
            });
        }

        console.log('Video generation operation started:', operation.name);

        // Poll the operation status until the video is ready
        let elapsedSeconds = 0;
        while (!operation.done) {
            await new Promise(resolve => setTimeout(resolve, 10000)); // Wait 10 seconds
            elapsedSeconds += 10;

            if (options?.onProgress) {
                options.onProgress(elapsedSeconds);
            }

            console.log(`Waiting for video generation... ${elapsedSeconds}s elapsed`);
            operation = await imagenClient.operations.get({ operation });

            // Timeout after 7 minutes (420 seconds)
            if (elapsedSeconds >= 420) {
                throw new Error('Video generation timeout: exceeded 7 minutes');
            }
        }

        console.log('Video generation complete!');

        // SDK response format (used for all modes now)
        if (!operation.response?.generatedVideos || operation.response.generatedVideos.length === 0) {
            throw new Error('No videos generated by Veo 3.1');
        }

        const generatedVideo = operation.response.generatedVideos[0];
        if (!generatedVideo?.video) {
            throw new Error('Invalid video data from Veo 3.1');
        }

        const videoUri = generatedVideo.video.uri;
        console.log('Video URI:', videoUri);

        // Download the video from URI
        if (!videoUri) {
            throw new Error('No video URI available in response');
        }

        const videoResponse = await fetch(videoUri, {
            headers: {
                'x-goog-api-key': process.env.GEMINI_API_KEY || ''
            }
        });

        if (!videoResponse.ok) {
            throw new Error(`Failed to download video: ${videoResponse.statusText}`);
        }

        const arrayBuffer = await videoResponse.arrayBuffer();
        const videoBuffer = Buffer.from(arrayBuffer);

        // Extract metadata (Veo 3.1 generates 8-second videos at 24fps)
        const duration = 8; // Default for Veo 3.1
        const resolution = '720p'; // Default resolution

        return {
            videoData: videoBuffer,
            mimeType: 'video/mp4',
            duration,
            resolution,
        };
    } catch (error) {
        console.error('Error generating video with Veo 3.1:', error);
        throw error;
    }
}

